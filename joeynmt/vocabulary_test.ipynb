{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constants.py\n",
    "https://github.com/joeynmt/joeynmt/blob/master/joeynmt/constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining global constants\n",
    "\"\"\"\n",
    "\n",
    "UNK_TOKEN = '<unk>'\n",
    "PAD_TOKEN = '<pad>'\n",
    "BOS_TOKEN = '<s>'\n",
    "EOS_TOKEN = '</s>'\n",
    "\n",
    "DEFAULT_UNK_ID = lambda: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_UNK_ID()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocabulary.py\n",
    "https://github.com/joeynmt/joeynmt/blob/master/joeynmt/vocabulary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict,Counter\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from torchtext.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Vocabulary module\n",
    "\"\"\"\n",
    "\n",
    "class Vocabulary:\n",
    "    '''Vocabulary represents mapping between tokens and indices. Vocabulary代表着tokens（标记）和indices（索引）之间的映射'''\n",
    "    def __init__(self, tokens:List[str]=None, file:str=None, encoding='utf-8',lower=False) ->None:\n",
    "        '''\n",
    "        Create vocabulary from list of tokens or file.从标记列表或文件来创建词表\n",
    "        Special tokens are added if not already in file or list.加入特殊标记，即使这些特殊标记没有出现在标记列表或文件中\n",
    "        File format: token with index i is in line i. 文件格式：索引为i的标记，对应文件中的第i行\n",
    "        :param tokens: list of tokens 标记列表\n",
    "        :param file: file to load vocabulary from 文件路径，将从该文件中载入词表\n",
    "        '''\n",
    "        # don't rename stoi and itos since needed for torchtext\n",
    "        # warning: stoi grows with unknown tokens, don't use for saving or size\n",
    "        \n",
    "        # special symbols\n",
    "        self.specials = [UNK_TOKEN, PAD_TOKEN, BOS_TOKEN, EOS_TOKEN]\n",
    "        self.stoi = defaultdict(DEFAULT_UNK_ID)\n",
    "        self.itos = []\n",
    "        if tokens is not None:\n",
    "            self._from_list(tokens)\n",
    "        if file is not None:\n",
    "            self._from_file(file,encoding = encoding,lower = lower)\n",
    "    def add_tokens(self, tokens:List[str],lower=False) ->None:\n",
    "        \"\"\"\n",
    "        Add list of tokens to vocabulary\n",
    "        :param tokens: list of tokens to add to the vocabulary\n",
    "        \"\"\"\n",
    "        for token in tokens:\n",
    "            new_index=len(self.itos)\n",
    "            if token not in self.itos:\n",
    "                token=token.strip()\n",
    "                if lower:\n",
    "                    token=token.lower()\n",
    "                self.itos.append(token)\n",
    "                self.stoi[token]=new_index\n",
    "        \n",
    "    def _from_list(self, tokens: List[str] = None,lower=False) -> None:\n",
    "        \"\"\"\n",
    "        Make vocabulary from list of tokens.\n",
    "        Tokens are assumed to be unique and pre-selected.\n",
    "        Special symbols are added if not in list.\n",
    "        :param tokens: list of tokens\n",
    "        \"\"\"\n",
    "        self.add_tokens(tokens=self.specials+tokens,lower=lower)\n",
    "        assert len(self.stoi) == len(self.itos)\n",
    "        \n",
    "    def _from_file(self, file: str,encoding='utf-8',lower=False) -> None:\n",
    "        \"\"\"\n",
    "        Make vocabulary from contents of file.\n",
    "        File format: token with index i is in line i.\n",
    "        :param file: path to file where the vocabulary is loaded from\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        with open(file, \"r\",encoding=encoding) as open_file:\n",
    "            for line in open_file:\n",
    "                tokens.append(line.strip(\"\\n\"))\n",
    "        self._from_list(tokens,lower=lower)\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return self.stoi.__str__()\n",
    "    \n",
    "    def to_file(self, file: str,encoding='utf-8') -> None:\n",
    "        \"\"\"\n",
    "        Save the vocabulary to a file, by writing token with index i in line i.\n",
    "        :param file: path to file where the vocabulary is written\n",
    "        \"\"\"\n",
    "        with open(file, \"w\",encoding=encoding) as open_file:\n",
    "            for t in self.itos:\n",
    "                open_file.write(\"{}\\n\".format(t))\n",
    "    \n",
    "    def is_unk(self, token: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether a token is covered by the vocabulary\n",
    "        :param token:\n",
    "        :return: True if covered, False otherwise\n",
    "        \"\"\"\n",
    "        return self.stoi[token] == DEFAULT_UNK_ID()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.itos)\n",
    "    \n",
    "    def _array_to_sentence(self, array: np.array, cut_at_eos=True, skip_pad=True) -> List[str]:\n",
    "        \"\"\"\n",
    "        1D array --> sentence\n",
    "        \n",
    "        Converts an array of IDs to a sentence, optionally cutting the result\n",
    "        off at the end-of-sequence token.\n",
    "        :param array: 1D array containing indices\n",
    "        :param cut_at_eos: cut the decoded sentences at the first <eos>\n",
    "        :param skip_pad: skip generated <pad> tokens\n",
    "        :return: list of strings (tokens)\n",
    "        \"\"\"\n",
    "        sentence = []\n",
    "        for i in array:\n",
    "            s = self.itos[i]\n",
    "            if cut_at_eos and s == EOS_TOKEN:\n",
    "                break\n",
    "            if skip_pad and s == PAD_TOKEN:\n",
    "                continue\n",
    "            sentence.append(s)\n",
    "        return sentence\n",
    "    \n",
    "    def _arrays_to_sentences(self, arrays: np.array, cut_at_eos=True,skip_pad=True) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        2D array --> sentences\n",
    "        \n",
    "        Convert multiple arrays containing sequences of token IDs to their\n",
    "        sentences, optionally cutting them off at the end-of-sequence token.\n",
    "        :param arrays: 2D array containing indices\n",
    "        :param cut_at_eos: cut the decoded sentences at the first <eos>\n",
    "        :param skip_pad: skip generated <pad> tokens\n",
    "        :return: list of list of strings (tokens)\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        for array in arrays:\n",
    "            sentences.append(self._array_to_sentence(array=array, cut_at_eos=cut_at_eos,skip_pad=skip_pad))\n",
    "        return sentences\n",
    "    \n",
    "    def array_to_sentence(self, array: np.array, cut_at_eos=True,skip_pad=True) -> List[List[str]]:\n",
    "        dim=len(array.shape)\n",
    "        if dim==1:\n",
    "            return self._array_to_sentence(array, cut_at_eos, skip_pad)\n",
    "        if dim==2:\n",
    "            return self._arrays_to_sentences(array, cut_at_eos, skip_pad)\n",
    "        else:\n",
    "            raise ValueError(f\"要求参数array必须是1D或2D的，但现在输入的参数array是{dim}D的！\")\n",
    "    def arrays_to_sentences(self, array: np.array, cut_at_eos=True,skip_pad=True) -> List[List[str]]:\n",
    "        return self.array_to_sentence(array,cut_at_eos,skip_pad)\n",
    "            \n",
    "    def build_vocab(field: str, max_size: int, min_freq: int=0, dataset: Dataset = None , vocab_file: str = None, encoding='utf-8',lower=False) -> Vocabulary:\n",
    "        \"\"\"\n",
    "        Builds vocabulary for a torchtext `field` from given`dataset` or\n",
    "        `vocab_file`.\n",
    "        :param field: attribute e.g. \"src\"\n",
    "        :param max_size: maximum size of vocabulary\n",
    "        :param min_freq: minimum frequency for an item to be included\n",
    "        :param dataset: dataset to load data for field from\n",
    "        :param vocab_file: file to store the vocabulary,\n",
    "            if not None, load vocabulary from here\n",
    "        :return: Vocabulary created from either `dataset` or `vocab_file`\n",
    "        \"\"\"\n",
    "\n",
    "        if vocab_file is not None:\n",
    "            # load it from file\n",
    "            vocab = Vocabulary(file=vocab_file, encoding=encoding, lower=lower)\n",
    "        elif dataset is not None:\n",
    "            # create newly\n",
    "            def filter_min(counter: Counter, min_freq: int):\n",
    "                \"\"\" Filter counter by min frequency \"\"\"\n",
    "                filtered_counter = Counter({t: c for t, c in counter.items() if c >= min_freq})\n",
    "                return filtered_counter\n",
    "\n",
    "            def sort_and_cut(counter: Counter, limit: int):\n",
    "                \"\"\" Cut counter to most frequent,\n",
    "                sorted numerically and alphabetically\"\"\"\n",
    "                # sort by frequency, then alphabetically\n",
    "                tokens_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "                tokens_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "                vocab_tokens = [i[0] for i in tokens_and_frequencies[:limit]]\n",
    "                return vocab_tokens\n",
    "\n",
    "            tokens = []\n",
    "            for i in dataset.examples:\n",
    "                if field == \"src\":\n",
    "                    tokens.extend(i.src)\n",
    "                elif field == \"trg\":\n",
    "                    tokens.extend(i.trg)\n",
    "\n",
    "            counter = Counter(tokens)\n",
    "            if min_freq > 0:\n",
    "                counter = filter_min(counter, min_freq)\n",
    "            vocab_tokens = sort_and_cut(counter, max_size)\n",
    "            assert len(vocab_tokens) <= max_size\n",
    "\n",
    "            vocab = Vocabulary(tokens=vocab_tokens)\n",
    "            assert len(vocab) <= max_size + len(vocab.specials)\n",
    "            assert vocab.itos[DEFAULT_UNK_ID()] == UNK_TOKEN\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"要求参数dataset或者vocab_file 至少有一个不为空！\")\n",
    "\n",
    "        # check for all except for UNK token whether they are OOVs\n",
    "        for s in vocab.specials[1:]:\n",
    "            assert not vocab.is_unk(s)\n",
    "\n",
    "        return vocab  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=Vocabulary()\n",
    "vocab2=Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab._from_file('untitled.txt')\n",
    "vocab2._from_file('untitled.txt',lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "要求参数dataset或者vocab_file 至少有一个不为空！",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-252-131e08eca2d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-249-2e6a2231d176>\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(field, max_size, min_freq, dataset, vocab_file, encoding, lower)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"要求参数dataset或者vocab_file 至少有一个不为空！\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# check for all except for UNK token whether they are OOVs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 要求参数dataset或者vocab_file 至少有一个不为空！"
     ]
    }
   ],
   "source": [
    "vocab.build_vocab(max_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " 'Make',\n",
       " 'vocabulary',\n",
       " 'from',\n",
       " 'list',\n",
       " 'of',\n",
       " 'tokens']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=vocab.build_vocab(max_size=3,vocab_file='untitled.txt', encoding='utf-8',lower=False)\n",
    "v.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " 'Make',\n",
       " 'vocabulary',\n",
       " 'from',\n",
       " 'list',\n",
       " 'of',\n",
       " 'tokens']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=vocab.build_vocab(max_size=3,vocab_file='untitled.txt')\n",
    "v.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "array=np.array([1,2,3])\n",
    "arrays=np.array([[1,2,3],[1,2,0]])\n",
    "array_3d=np.array([[[1,2,3],[1,2,0]],[[1,2,3],[1,2,0]]])\n",
    "array_0d=np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2, 3), (0,))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_3d.shape,array_0d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3,), (2, 3), (2, 2, 3))"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.shape,arrays.shape,array_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(array.shape),len(arrays.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<s>'], ['<s>'])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1=vocab.array_to_sentence(array)\n",
    "s2=vocab.arrays_to_sentences(array)\n",
    "s1,s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['<s>'], ['<s>', '<unk>']], [['<s>'], ['<s>', '<unk>']])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1=vocab.array_to_sentence(arrays)\n",
    "s2=vocab.arrays_to_sentences(arrays)\n",
    "s1,s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<s>', '</s>']"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.arrays_to_sentences(array, cut_at_eos=False,skip_pad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<s>', '</s>']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.array_to_sentence(array, cut_at_eos=False,skip_pad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<pad>', '<s>', '</s>'], ['<pad>', '<s>', '<unk>']]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss=vocab.arrays_to_sentences(arrays, cut_at_eos=False,skip_pad=False)\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<pad>', '<s>', '</s>'], ['<pad>', '<s>', '<unk>']]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.array_to_sentence(arrays, cut_at_eos=False,skip_pad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "要求参数array必须是1D或2D的，但现在输入的参数array是3D的！",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-7f877e0574a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_to_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_at_eos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-249-2e6a2231d176>\u001b[0m in \u001b[0;36marray_to_sentence\u001b[0;34m(self, array, cut_at_eos, skip_pad)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arrays_to_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_at_eos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"要求参数array必须是1D或2D的，但现在输入的参数array是{dim}D的！\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0marrays_to_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_at_eos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_to_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcut_at_eos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 要求参数array必须是1D或2D的，但现在输入的参数array是3D的！"
     ]
    }
   ],
   "source": [
    "vocab.array_to_sentence(array_3d, cut_at_eos=False,skip_pad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "要求参数array必须是1D或2D的，但现在输入的参数array是3D的！",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-268-0bbee9b15ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays_to_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_at_eos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-249-2e6a2231d176>\u001b[0m in \u001b[0;36marrays_to_sentences\u001b[0;34m(self, array, cut_at_eos, skip_pad)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"要求参数array必须是1D或2D的，但现在输入的参数array是{dim}D的！\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0marrays_to_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_at_eos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_to_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcut_at_eos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-2e6a2231d176>\u001b[0m in \u001b[0;36marray_to_sentence\u001b[0;34m(self, array, cut_at_eos, skip_pad)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arrays_to_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_at_eos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"要求参数array必须是1D或2D的，但现在输入的参数array是{dim}D的！\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0marrays_to_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_at_eos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_to_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcut_at_eos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 要求参数array必须是1D或2D的，但现在输入的参数array是3D的！"
     ]
    }
   ],
   "source": [
    "vocab.arrays_to_sentences(array_3d, cut_at_eos=False,skip_pad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.arrays_to_sentences(array_0d, cut_at_eos=False,skip_pad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.array_to_sentence(array_0d, cut_at_eos=False,skip_pad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"defaultdict(<function <lambda> at 0x7fb54d0fadd0>, {'<unk>': 0, '<pad>': 1, '<s>': 2, '</s>': 3, 'make': 4, 'vocabulary': 5, 'from': 6, 'list': 7, 'of': 8, 'tokens': 9})\""
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(vocab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             '<s>': 2,\n",
       "             '</s>': 3,\n",
       "             'Make': 4,\n",
       "             'vocabulary': 5,\n",
       "             'from': 6,\n",
       "             'list': 7,\n",
       "             'of': 8,\n",
       "             'tokens': 9})"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<unk>',\n",
       "  '<pad>',\n",
       "  '<s>',\n",
       "  '</s>',\n",
       "  'Make',\n",
       "  'vocabulary',\n",
       "  'from',\n",
       "  'list',\n",
       "  'of',\n",
       "  'tokens'],\n",
       " ['<unk>',\n",
       "  '<pad>',\n",
       "  '<s>',\n",
       "  '</s>',\n",
       "  'make',\n",
       "  'vocabulary',\n",
       "  'from',\n",
       "  'list',\n",
       "  'of',\n",
       "  'tokens'])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.itos,vocab2.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.add_tokens(vocab.specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<unk>',\n",
       "  '<pad>',\n",
       "  '<s>',\n",
       "  '</s>',\n",
       "  'Make',\n",
       "  'vocabulary',\n",
       "  'from',\n",
       "  'list',\n",
       "  'of',\n",
       "  'tokens'],)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.itos,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, '<s>')"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi['<unk>'],vocab.itos[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             '<s>': 2,\n",
       "             '</s>': 3,\n",
       "             'Make': 4,\n",
       "             'vocabulary': 5,\n",
       "             'from': 6,\n",
       "             'list': 7,\n",
       "             'of': 8,\n",
       "             'tokens': 9})"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "specials = [UNK_TOKEN, PAD_TOKEN, BOS_TOKEN, EOS_TOKEN]\n",
    "stoi = defaultdict(DEFAULT_UNK_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<s>', '</s>']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi['3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two\n",
      "0\n",
      "set()\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "dict1 = defaultdict(int)\n",
    "dict2 = defaultdict(set)\n",
    "dict3 = defaultdict(str)\n",
    "dict4 = defaultdict(list)\n",
    "dict1[2] ='two'\n",
    "\n",
    "print(dict1[2])\n",
    "print(dict1[1])\n",
    "print(dict2[1])\n",
    "print(dict3[1])\n",
    "print(dict4[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {2: 'two', 1: 0})"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dltest",
   "language": "python",
   "name": "dltest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
